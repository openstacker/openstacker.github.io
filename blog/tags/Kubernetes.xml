<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:base="https://openstacker.github.io">
  <title>
    Blog  </title>
    <link href="https://openstacker.github.io/blog/tags/Kubernetes.xml" rel="self" />
  
    <link href="https://openstacker.github.io/blog/"/>
  
    
  <updated>2018-10-01T15:01:11Z</updated>

  <id>https://openstacker.github.io/blog/tags/Kubernetes.xml</id>

      <entry>
    <title type="html">Journey of Magnum to Production</title>
    <author><name>Feilong Wang</name></author>
    <link href="https://openstacker.github.io/blog/2018/magnum-to-production"/>
    <updated>2018-10-01T10:37:00Z</updated>
    <published>2018-10-01T10:37:00Z</published>
    <id>/blog/2018/magnum-to-production</id>
        <category   scheme="/blog/tags"
                term="Magnum"
                label="Magnum" />
        <category   scheme="/blog/tags"
                term="Fedora-Atomic"
                label="Fedora-Atomic" />
        <category   scheme="/blog/tags"
                term="Cloud-init"
                label="Cloud-Init" />
        <category   scheme="/blog/tags"
                term="Kubernetes"
                label="Kubernetes" />
    
    <content type="html">
                  &lt;p&gt;在上一篇&lt;a href=&#34;http://openstacker.github.io/blog/2017/magnum-architecture&#34;&gt;Magnum 入坑指南之一&lt;/a&gt;和&lt;a href=&#34;http://openstacker.github.io/blog/2018/magnum-2&#34;&gt;Magnum 入坑指南之二&lt;/a&gt;里
已经介绍了Magnum的基本安装和使用。这篇文章主要用来记录我们在将Magnum部署到生产环境时所遇到的问题，也好作为今年Berlin Summit的presentation 材料。&lt;/p&gt;
&lt;p&gt;到目前为止，我们在上prod/preprod的过程中遇到了两个问题。&lt;/p&gt;
&lt;h2&gt;anti-affinity policy&lt;/h2&gt;
&lt;p&gt;这个其实是我去年做的一个feature, 简单来说就是在创建k8s cluster时，将master节点和node节点放到一个server group里，然后设置server group的affinity policy 为soft-anti-affinity或者
anti-affinity, 从而使各个节点在schedule时能够被创建到不同的物理机上，避免node节点拥挤到一个节点上。&lt;/p&gt;
&lt;p&gt;但是我们的环境里暂时还不支持soft-anti-affinity, 这个我们实现没有测试到，还好可以直接在Magnum cluster show里面看到所以并没有浪费时间。&lt;/p&gt;
&lt;h2&gt;Cloud-init 的奇怪问题&lt;/h2&gt;
&lt;p&gt;接下来，我们遇到了cloud-init的问题。root cause说来也简单，就是我们的preprod太慢了，慢导致了很多问题，只不过各种问题的表象没有看起来那么直观。
回到我们所遇到的问题，Magnum所创建的instance无法注入keypair, 导致无法ssh到instance内debug。当然问题看起来很简单，公钥没有成功注入。接下来我们发现公钥没有注入的原因就是
cloud-init初始化就失败了。不得已，我又专门build了一个带用户名密码的 Fedora Atomic 镜像。重新创建 k8s cluster, 然后登录虚机，查看cloud-init.log 发现问题的根源在于
当 cloud-init 在试图和 Nova的metadata API连接时出现了超时，默认的timeout是5秒钟。通过查阅文档发现，可以通过更改/etc/cloud/cloud.cfg, 更改timeout时间，具体修改如下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;datasource:&lt;br /&gt;  OpenStack:&lt;br /&gt;    metadata_urls: &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;http://169.254.169.254:80&amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;&lt;br /&gt;    dsmode: net&lt;br /&gt;    timeout: &lt;span class=&#34;m&#34;&gt;50&lt;/span&gt;&lt;br /&gt;    max_wait: &lt;span class=&#34;m&#34;&gt;120&lt;/span&gt;&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
然而，这种方式并不能完全解决问题，具体看下面的log&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;2018&lt;/span&gt;-09-30 &lt;span class=&#34;m&#34;&gt;09&lt;/span&gt;:34:16,875 - handlers.py&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;DEBUG&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;: start: init-network/search-OpenStack: searching &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; network data from DataSourceOpenStack&lt;br /&gt;&lt;span class=&#34;m&#34;&gt;2018&lt;/span&gt;-09-30 &lt;span class=&#34;m&#34;&gt;09&lt;/span&gt;:34:16,875 - __init__.py&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;DEBUG&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;: Seeing &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; we can get any data from &amp;lt;class &lt;span class=&#34;s1&#34;&gt;&amp;#39;cloudinit.sources.DataSourceOpenStack.DataSourceOpenStack&amp;#39;&lt;/span&gt;&amp;gt;&lt;br /&gt;&lt;span class=&#34;m&#34;&gt;2018&lt;/span&gt;-09-30 &lt;span class=&#34;m&#34;&gt;09&lt;/span&gt;:34:16,876 - url_helper.py&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;DEBUG&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;: &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;/1&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; open &lt;span class=&#34;s1&#34;&gt;&amp;#39;http://169.254.169.254:80/openstack&amp;#39;&lt;/span&gt; with &lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;url&amp;#39;&lt;/span&gt;: &lt;span class=&#34;s1&#34;&gt;&amp;#39;http://169.254.169.254:80/openstack&amp;#39;&lt;/span&gt;, &lt;span class=&#34;s1&#34;&gt;&amp;#39;allow_redirects&amp;#39;&lt;/span&gt;: True, &lt;span class=&#34;s1&#34;&gt;&amp;#39;method&amp;#39;&lt;/span&gt;: &lt;span class=&#34;s1&#34;&gt;&amp;#39;GET&amp;#39;&lt;/span&gt;, &lt;span class=&#34;s1&#34;&gt;&amp;#39;timeout&amp;#39;&lt;/span&gt;: &lt;span class=&#34;m&#34;&gt;50&lt;/span&gt;.0, &lt;span class=&#34;s1&#34;&gt;&amp;#39;headers&amp;#39;&lt;/span&gt;: &lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;User-Agent&amp;#39;&lt;/span&gt;: &lt;span class=&#34;s1&#34;&gt;&amp;#39;Cloud-Init/0.7.9&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;}}&lt;/span&gt; configuration&lt;br /&gt;&lt;span class=&#34;m&#34;&gt;2018&lt;/span&gt;-09-30 &lt;span class=&#34;m&#34;&gt;09&lt;/span&gt;:34:28,873 - url_helper.py&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;DEBUG&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;: Read from http://169.254.169.254:80/openstack &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;200&lt;/span&gt;, 50b&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; after &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; attempts&lt;br /&gt;&lt;span class=&#34;m&#34;&gt;2018&lt;/span&gt;-09-30 &lt;span class=&#34;m&#34;&gt;09&lt;/span&gt;:34:28,874 - DataSourceOpenStack.py&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;DEBUG&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;: Using metadata source: &lt;span class=&#34;s1&#34;&gt;&amp;#39;http://169.254.169.254:80&amp;#39;&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;m&#34;&gt;2018&lt;/span&gt;-09-30 &lt;span class=&#34;m&#34;&gt;09&lt;/span&gt;:34:28,874 - url_helper.py&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;DEBUG&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;: &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;/6&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; open &lt;span class=&#34;s1&#34;&gt;&amp;#39;http://169.254.169.254:80/openstack&amp;#39;&lt;/span&gt; with &lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;url&amp;#39;&lt;/span&gt;: &lt;span class=&#34;s1&#34;&gt;&amp;#39;http://169.254.169.254:80/openstack&amp;#39;&lt;/span&gt;, &lt;span class=&#34;s1&#34;&gt;&amp;#39;allow_redirects&amp;#39;&lt;/span&gt;: True, &lt;span class=&#34;s1&#34;&gt;&amp;#39;method&amp;#39;&lt;/span&gt;: &lt;span class=&#34;s1&#34;&gt;&amp;#39;GET&amp;#39;&lt;/span&gt;, &lt;span class=&#34;s1&#34;&gt;&amp;#39;timeout&amp;#39;&lt;/span&gt;: &lt;span class=&#34;m&#34;&gt;5&lt;/span&gt;.0, &lt;span class=&#34;s1&#34;&gt;&amp;#39;headers&amp;#39;&lt;/span&gt;: &lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;User-Agent&amp;#39;&lt;/span&gt;: &lt;span class=&#34;s1&#34;&gt;&amp;#39;Cloud-Init/0.7.9&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;}}&lt;/span&gt; configuration&lt;br /&gt;&lt;span class=&#34;m&#34;&gt;2018&lt;/span&gt;-09-30 &lt;span class=&#34;m&#34;&gt;09&lt;/span&gt;:34:33,885 - openstack.py&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;DEBUG&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;: Unable to &lt;span class=&#34;nb&#34;&gt;read&lt;/span&gt; openstack versions from http://169.254.169.254:80 due to: HTTPConnectionPool&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;host&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;169.254.169.254&amp;#39;&lt;/span&gt;, &lt;span class=&#34;nv&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;80&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;: Read timed out. &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;read&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;timeout&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;5&lt;/span&gt;.0&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;m&#34;&gt;2018&lt;/span&gt;-09-30 &lt;span class=&#34;m&#34;&gt;09&lt;/span&gt;:34:33,885 - openstack.py&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;DEBUG&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;: Selected version &lt;span class=&#34;s1&#34;&gt;&amp;#39;latest&amp;#39;&lt;/span&gt; from &lt;span class=&#34;o&#34;&gt;[]&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;m&#34;&gt;2018&lt;/span&gt;-09-30 &lt;span class=&#34;m&#34;&gt;09&lt;/span&gt;:34:33,886 - url_helper.py&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;DEBUG&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;: &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;/6&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; open &lt;span class=&#34;s1&#34;&gt;&amp;#39;http://169.254.169.254:80/openstack/latest/meta_data.json&amp;#39;&lt;/span&gt; with &lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;url&amp;#39;&lt;/span&gt;: &lt;span class=&#34;s1&#34;&gt;&amp;#39;http://169.254.169.254:80/openstack/latest/meta_data.json&amp;#39;&lt;/span&gt;, &lt;span class=&#34;s1&#34;&gt;&amp;#39;allow_redirects&amp;#39;&lt;/span&gt;: True, &lt;span class=&#34;s1&#34;&gt;&amp;#39;method&amp;#39;&lt;/span&gt;: &lt;span class=&#34;s1&#34;&gt;&amp;#39;GET&amp;#39;&lt;/span&gt;, &lt;span class=&#34;s1&#34;&gt;&amp;#39;timeout&amp;#39;&lt;/span&gt;: &lt;span class=&#34;m&#34;&gt;5&lt;/span&gt;.0, &lt;span class=&#34;s1&#34;&gt;&amp;#39;headers&amp;#39;&lt;/span&gt;: &lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;User-Agent&amp;#39;&lt;/span&gt;: &lt;span class=&#34;s1&#34;&gt;&amp;#39;Cloud-Init/0.7.9&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;}}&lt;/span&gt; configuration&lt;br /&gt;&lt;span class=&#34;m&#34;&gt;2018&lt;/span&gt;-09-30 &lt;span class=&#34;m&#34;&gt;09&lt;/span&gt;:34:38,894 - openstack.py&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;DEBUG&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;: Failed reading mandatory path http://169.254.169.254:80/openstack/latest/meta_data.json due to: HTTPConnectionPool&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;host&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;169.254.169.254&amp;#39;&lt;/span&gt;, &lt;span class=&#34;nv&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;80&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;: Read timed out. &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;read&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;timeout&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;5&lt;/span&gt;.0&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;m&#34;&gt;2018&lt;/span&gt;-09-30 &lt;span class=&#34;m&#34;&gt;09&lt;/span&gt;:34:38,894 - util.py&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;DEBUG&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;: Crawl of openstack metadata service took &lt;span class=&#34;m&#34;&gt;10&lt;/span&gt;.020 seconds&lt;br /&gt;&lt;span class=&#34;m&#34;&gt;2018&lt;/span&gt;-09-30 &lt;span class=&#34;m&#34;&gt;09&lt;/span&gt;:34:38,895 - handlers.py&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;DEBUG&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;: finish: init-network/search-OpenStack: SUCCESS: no network data found from DataSourceOpenStack&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
这个timeout 只对第一个request 起了作用，后面的request仍然是按照 5 秒钟作为timeout。我研究到这里的时候已经后半夜两点了，不想再研究只对第一个request 起作用了，所以我决定直接改代码。&lt;/p&gt;
&lt;p&gt;然而我发现在 Fedora Atomic, 即使你是root 用户，也是不能直接修改 cloud-init的代码的，这部分代码是只读的。&lt;/p&gt;
&lt;p&gt;要修改这部分只读的代码，需要先知道当前deploy的host status, 可以通过如下命令获得：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;fedora@k8s-cluster-1-adbneq5un4ok-master-0 log&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;$ sudo atomic host status&lt;br /&gt;State: idle&lt;br /&gt;Deployments:&lt;br /&gt;* fedora-atomic:fedora/27/x86_64/atomic-host&lt;br /&gt;                   Version: &lt;span class=&#34;m&#34;&gt;27&lt;/span&gt;.81 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;2018&lt;/span&gt;-02-12 &lt;span class=&#34;m&#34;&gt;17&lt;/span&gt;:50:48&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;br /&gt;                    Commit: b25bde0109441817f912ece57ca1fc39efc60e6cef4a7a23ad9de51b1f36b742&lt;br /&gt;              GPGSignature: Valid signature by 860E19B0AFA800A1751881A6F55E7430F5282EE4&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
接下来再通过上面命令得到的commit ID, 通过chroot 命令切换到相应的commit下，这时候就可以读写了。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;root@fedora-atomic-27-x86_64 log&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;# chroot /sysroot/ostree/deploy/fedora-atomic/deploy/b25bde0109441817f912ece57ca1fc39efc60e6cef4a7a23ad9de51b1f36b742.0/&lt;/span&gt;&lt;br /&gt;bash-4.4# vi /usr/lib/python3.6/site-packages/cloudinit/url_helper.py &lt;br /&gt;bash-4.4# &lt;span class=&#34;nb&#34;&gt;exit&lt;/span&gt;&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
未完待续&lt;/p&gt;      
          </content>
  </entry>
    <entry>
    <title type="html">calico网络问题的debug过程</title>
    <author><name>Feilong Wang</name></author>
    <link href="https://openstacker.github.io/blog/2018/calico-debug"/>
    <updated>2018-03-22T14:37:00Z</updated>
    <published>2018-03-22T14:37:00Z</published>
    <id>/blog/2018/calico-debug</id>
        <category   scheme="/blog/tags"
                term="Calico"
                label="Calico" />
        <category   scheme="/blog/tags"
                term="Kubernetes"
                label="Kubernetes" />
    
    <content type="html">
                  &lt;p&gt;这篇blog 主要是用来记录我最近在开发Magnum的过程中，集成calico和kuberntes的过程中所遇到的一些问题，以及我对calico的一些粗浅的理解。&lt;/p&gt;
&lt;h2&gt;什么是Calico&lt;/h2&gt;
&lt;p&gt;calico 同flannel类似，主要为容器提供网络支持。下面的文字摘自calico官方网站对其的描述。&lt;/p&gt;
&lt;p&gt;Calico provides secure network connectivity for containers and virtual machine workloads.&lt;/p&gt;
&lt;p&gt;Calico creates and manages a flat layer 3 network, assigning each workload a fully routable IP address. Workloads can communicate without IP encapsulation or network address translation for bare metal performance, easier troubleshooting, and better interoperability. In environments that require an overlay, Calico uses IP-in-IP tunneling or can work with other overlay networking such as flannel.&lt;/p&gt;
&lt;p&gt;Calico also provides dynamic enforcement of network security rules. Using Calico’s simple policy language, you can achieve fine-grained control over communications between containers, virtual machine workloads, and bare metal host endpoints.&lt;/p&gt;
&lt;h2&gt;为什么需要Calico&lt;/h2&gt;
&lt;p&gt;通常提到calico必然提到网络隔离，这是calico 区别于Flannel的主要一点。因此，使用Calico主要是为了实现不同namespaces的网络隔离。&lt;/p&gt;
&lt;h2&gt;calico 网络问题的debug过程&lt;/h2&gt;
&lt;p&gt;这个问题实际上是我最近调试 calico 和 kubernetes所遇到的一个实际问题。具体症状是这样的：通过Magnum创建k8s 集群后，
kuberntes dashboard pod和coreDNS pod周期性的重启，而且比较频繁。其他pod正常。&lt;/p&gt;
&lt;p&gt;下面是我的分析过程，参考了&lt;a href=&#34;https://www.lijiaocn.com/%E6%96%B9%E6%B3%95/2017/08/18/calico-network-problem-resove.html&#34;&gt;这篇文章&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;判断在pod重启之前是否有问题
   这个和上面lijiao的博客里所介绍的分析步骤一样，就是看端到端的连通性是否有问题。测试之后发现没问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;回到pod的原理本身，重启说明 liveness 健康检查失败，根源在于网络失效，考虑到calico的原因，基本可以确定是calico 引起的&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;周期性问题
   经过多次不断实验，发现重启的频率基本在8分钟左右，但在重启之前网络是好的。那么说明，一个东西在破坏，一个东西在修复。去calico 的 slack里channel里询问了他们的工程师之后得到了启发，可能和NetworkManager有关系。因为我们用的Fedora Atomic image里默认仍然启用的是NetworkManager, 它会做一些周期性的检查，清理那些它认为不正确的device。遗憾的是，calico 所创建的某些虚拟device可能就在它认为不正确的范围内。造成的结果
就是NetworkManager不断去删，而calico node不断去修复，多么蛋疼的相爱相杀。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最终的解决办法是在Magnum里修改NetworkManager的配置（/etc/NetworkManager/NetworkManager.conf），让它忽略那些Magnum创建的devices。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;keyfile&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;&lt;br /&gt;unmanaged-devices&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;interface-name:cali*&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;interface-name:tunl*&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
具体内容请参考我给calico 提交的document PR https://github.com/projectcalico/calico/commit/a676f8ceb7371182acdd0f949c4d08ae992ac738&lt;/p&gt;
&lt;h2&gt;在K8s中使用Calico的一些“问题”&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;k8s master node 到底需不需要 calico-node&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这个根据我对calico的理解和实际的测试，master node 是需要calico-node的，否则会导致master和node之间的pod无法通信。这么说其实不准确，准确的说法应该是 从master无法访问到其他node上所运行的pod，例如dashboard。例如 dashboard如果运行在某一（几）个node上，那么通过kube-proxy 是需要从master访问到node上运行的 dashboard的pod的。所以如果这时候master上没有calico-node的话，kube-proxy不知道该把包转发到哪个node上去。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;pod 和 k8s api server 的通信问题&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这个问题我已经忘记当初为什么写在这里了，简单来说pod理论上不需要和api server通信。但如果是特殊的功能性的pod, 那可能是需要的。&lt;/p&gt;      
          </content>
  </entry>
    <entry>
    <title type="html">Magnum 入坑指南之二</title>
    <author><name>Feilong Wang</name></author>
    <link href="https://openstacker.github.io/blog/2018/magnum-2"/>
    <updated>2018-03-19T07:37:00Z</updated>
    <published>2018-03-19T07:37:00Z</published>
    <id>/blog/2018/magnum-2</id>
        <category   scheme="/blog/tags"
                term="Magnum"
                label="Magnum" />
        <category   scheme="/blog/tags"
                term="Kubernetes"
                label="Kubernetes" />
    
    <content type="html">
                  &lt;p&gt;截止到这篇blog动笔时，当前的OpenStack master版本是Rocky，目前Magnum所支持的Kubernetes的扩展功能有：
dashboard, DNS, proxy, 基于 Heapster的监控，基于Prometheus的监控，基于traefik的Ingress。此外还
支持两种network driver: flannel 和 calico。关于network driver部分，我后续会再专门写一篇
阐述。&lt;/p&gt;
&lt;p&gt;在上一篇&lt;a href=&#34;http://openstacker.github.io/blog/2017/magnum-architecture&#34;&gt;Magnum 入坑指南之一&lt;/a&gt;里已经介绍了Magnum的基本安装和使用，
其实说kubernetes是坑，以我做openstack多年的经验来说不算过分，kubernetes本身经过多年的发展，已经日趋稳定，但是围绕在它周边的众多
附加服务之间的各种组合，真的是让人头大。不同服务之间的不同版本也许完全不能工作，同一个服务的不同版本因为快速迭代可能并不兼容，凡此种种。
所以我现在写的这些也许过些日子回过头来看都很容易，但此时此刻，要想搭建一个production ready的k8s集群真的不是一般普通用户能轻松搞定的。
当然由此也体现出此类 managed k8s service 的价值所在。&lt;/p&gt;
&lt;p&gt;书接上文，这篇主要说说 Magnum里面基于k8s所提供的其他几个周边服务。&lt;/p&gt;
&lt;h2&gt;k8s dashboard&lt;/h2&gt;
&lt;p&gt;Mangum 在部署 k8s 时默认会安装 k8s dashboard, 访问的方式也很简单，可以通过 k8s 
proxy 从本机的浏览器进行访问。首先，在本机运行&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;kubectl proxy&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
然后在本地的浏览器里输入：http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/&lt;/p&gt;
&lt;h2&gt;Prometheus 监控&lt;/h2&gt;
&lt;p&gt;不同于其他几个服务，Prometheus在Magnum中是需要通过label enable的，默认并不开启。如果要开启，则需要在template 或者创建cluster的时候指定
lable &#34;&#34;prometheus_monitoring=true&#34;&#34;。在我写这篇文章时，Magnum中的Prometheus存在一个Bug，原因是缺失RBAC相关信息，Patch在这里
https://review.openstack.org/553654 截止到目前 Magnum中的Prometheus 仍然采用NodePort的方式访问，默认端口是30900, 所以在我的测试
环境中可以直接通过 http://&amp;lt; node&#39;s floating IP &amp;gt;:30900/访问Prometheus的dashboard。这里值得注意的是，判断是否能正确拿到数据的标准就是
在界面上能否绘制某个指标的图形，而在此之前最好先检查 Status -&amp;gt; Targets里是否能看到数据。否则Targets里拿不到数据的话，是无法绘制图形的。&lt;/p&gt;      
          </content>
  </entry>
    <entry>
    <title type="html">Magnum 入坑指南之一</title>
    <author><name>Feilong Wang</name></author>
    <link href="https://openstacker.github.io/blog/2017/magnum-architecture"/>
    <updated>2017-06-23T12:01:00Z</updated>
    <published>2017-06-23T12:01:00Z</published>
    <id>/blog/2017/magnum-architecture</id>
        <category   scheme="/blog/tags"
                term="Magnum"
                label="Magnum" />
        <category   scheme="/blog/tags"
                term="OpenStack"
                label="Openstack" />
        <category   scheme="/blog/tags"
                term="Architecture"
                label="Architecture" />
        <category   scheme="/blog/tags"
                term="Kubernetes"
                label="Kubernetes" />
    
    <content type="html">
                  &lt;p&gt;现在容器大火，OpenStack走下坡路, Magnum 作为OpenStack中的容器相关项目最多也就算是不温不火吧。但实际上，很多客户的应用仍然是传统的Web应用，尤其在新西兰市场，很多客户也只是刚刚把应用从物理机迁移到我们的云上，实际使用上也仅仅是租个虚机，然后就不折腾了。离真正的原生云应用，或者容器化都有不短的距离。&lt;/p&gt;
&lt;h2&gt;架构&lt;/h2&gt;
&lt;p&gt;本来想先介绍架构，但其实Magnum的架构其实很简单，倒是涉及到容器和虚机网络的网络架构比较复杂。我后面会补一张图来解释网络架构．Magnum的架构简单来说就是通过Heat去安装指定的COE. 周边涉及的网络和存储也都是通过Heat来创建的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/media/images/blog/2017/800px-Magnum_architecture.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图引自 Magnum wiki(https://wiki.openstack.org/wiki/Magnum)&lt;/p&gt;
&lt;h2&gt;基本操作&lt;/h2&gt;
&lt;p&gt;通常来说，从头创建一个 COE cluster 需要以下步骤：&lt;/p&gt;
&lt;p&gt;１. 创建 keypair&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;openstack keypair create feilong --public-key .ssh/id_rsa.pub&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
2. 创建需要的镜像&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;openstack image create fedora-atomic --disk-format qcow2 --container-format bare --property &lt;span class=&#34;nv&#34;&gt;os_distro&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;fedora-atomic --file ./Fedora-Atomic-26-20170723.0.x86_64.qcow2&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
创建镜像时注意指定 os_distro 属性，大小写敏感，目前支持的情况如下表所示：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;+-------------------+-------------------------+&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; COE               &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; os_distro               &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;+-------------------+-------------------------+&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; Kubernetes        &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; fedora-atomic, coreos   &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;+-------------------+-------------------------+&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; Swarm             &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; fedora-atomic           &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;+-------------------+-------------------------+&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; Mesos             &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; ubuntu                  &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;+-------------------+-------------------------+&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
3. Create cluster template&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;openstack coe cluster template create --name k8s --image 2a21599f-a42f-4b12-9888-ae0600168f3c --keypair feilong --flavor c1.c1r2 --master-flavor c1.c1r2 --coe kubernetes --external-network public --fixed-network &amp;lt;NETWORK ID&amp;gt;&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
4. Create cluster&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;openstack coe cluster create --name k8scluster --cluster-template k8s --node-count &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; --timeout &lt;span class=&#34;m&#34;&gt;30&lt;/span&gt; --label &lt;span class=&#34;nv&#34;&gt;kube_tag&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;v1.9.3&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2&gt;Debug&lt;/h2&gt;
&lt;p&gt;Magnum的debug　分两部分，一部分是OpenStack层面的debug, 一部分是COE层面的debug。&lt;/p&gt;
&lt;p&gt;先说 OpenStack 层面的 debug, 如果是 devstack, 那么目前 devstack里的服务都是通过sytemd管理，那么起停相关服务的命令需要通过：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;systemctl restart/start/stop devstack@magnum-api&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
而查看 log，则需要通过 journalctl&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;journalctl -u devstack@magnum-api&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
OpenStack 部分的debug 比较简单，和debug OpenStack其他服务基本一致。Magnum　里遇到问题通常是在虚机里实际部署 COE 时失败，这时就需要 ssh 到虚机里去查看失败的原因．Magnum 中 COE的部署是通过 Heat的 Software Deployment执行的。Heat 会将事先准备好的脚本按顺序拷贝到虚机中，具体存放在 &lt;code&gt;/var/lib/cloud/instances/&amp;lt;instance_id&amp;gt;/scripts&lt;/code&gt; 因此debug起来也比较简单，如果在 &lt;code&gt;/var/log/cloud-init-output.log&lt;/code&gt; 和 &lt;code&gt;/var/log/cloud-init.log&lt;/code&gt; 中看到某个脚本比如 part-008 失败了，那么只需要手工执行 part-008 这个脚本，向里面加入简单的log 即可很容易的分析出失败的原因。&lt;/p&gt;
&lt;p&gt;此外，还有一部分脚本是通过 &lt;code&gt;heat-container-agent&lt;/code&gt; 来运行的，这部分脚本存放在 /var/lib/heat-config/heat-config-script, 而如果想查看这些脚本执行的log情况，则需要通过 &lt;code&gt;journalctl -u heat-container-agent&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;至此，假设你已经解决了所有的问题，在OpenStack 能看到COE Cluster 成功创建，状态为 Create Complete。那么基本上意味着 OpenStack部分是正常的。运行 &lt;code&gt;openstack coe cluster list&lt;/code&gt; 得到运行结果应该是这样的：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;feilong@feilong-ThinkPad-X1-Carbon-2nd:~/devstack$ openstack coe cluster list&lt;br /&gt;+--------------------------------------+------------+---------+------------+--------------+-----------------+&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; uuid                                 &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; name       &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; keypair &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; node_count &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; master_count &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; status          &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;+--------------------------------------+------------+---------+------------+--------------+-----------------+&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; 9147e750-164a-4f4d-83cc-e5bf994b8098 &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; k8scluster &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; feilong &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;          &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;            &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; CREATE_COMPLETE &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;+--------------------------------------+------------+---------+------------+--------------+-----------------+&lt;br /&gt;feilong@feilong-ThinkPad-X1-Carbon-2nd:~/devstack$ openstack coe cluster show k8scluster&lt;br /&gt;+---------------------+------------------------------------------------------------+&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; Field               &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; Value                                                      &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;+---------------------+------------------------------------------------------------+&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; status              &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; CREATE_COMPLETE                                            &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; cluster_template_id &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; a6822337-1f5b-4266-8967-ff99d8cdb82c                       &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; node_addresses      &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;u&lt;span class=&#34;s1&#34;&gt;&amp;#39;172.24.4.13&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;                                           &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; uuid                &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; 9147e750-164a-4f4d-83cc-e5bf994b8098                       &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; stack_id            &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; a5aa62ee-fb44-4d76-b225-38811adab0a9                       &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; status_reason       &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; Stack CREATE completed successfully                        &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; created_at          &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;2018&lt;/span&gt;-02-23T02:30:50+00:00                                  &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; updated_at          &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;2018&lt;/span&gt;-02-23T02:42:59+00:00                                  &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; coe_version         &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; v1.9.3                                                     &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; labels              &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;u&lt;span class=&#34;s1&#34;&gt;&amp;#39;kube_tag&amp;#39;&lt;/span&gt;: u&lt;span class=&#34;s1&#34;&gt;&amp;#39;v1.9.3&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;                                   &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; faults              &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;                                                            &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; keypair             &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; feilong                                                    &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; api_address         &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; https://172.24.4.3:6443                                    &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; master_addresses    &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;u&lt;span class=&#34;s1&#34;&gt;&amp;#39;172.24.4.3&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;                                            &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; create_timeout      &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;120&lt;/span&gt;                                                        &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; node_count          &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;                                                          &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; discovery_url       &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; https://discovery.etcd.io/b1c310361853d748444c2c48a814ce4d &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; master_count        &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;                                                          &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; container_version   &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;.12.6                                                     &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; name                &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; k8scluster                                                 &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; master_flavor_id    &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; ds1G                                                       &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; flavor_id           &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; ds1G                                                       &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;br /&gt;+---------------------+------------------------------------------------------------+&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
接下来，则需要验证 Kubernetes 自身的功能。这部分其实是最复杂的，当然也可能因为我个人对 k8s 还不够熟悉，而对 OpenStack 本身已经比较熟悉的关系。下面是我总结的在 debug k8s 基本功能时的一些简单的汇总。后面会陆续再单独开篇记录我学习 k8s 的经验。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;访问 API/Dashboard&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在 Magnum 中访问 k8s API 分两种方式，一种是从虚机内部访问，一种是本机远程访问。从虚机内部访问，没有任何限制，可以直接运行 kubectl 的各种命令。如果是从本机远程访问，则需要先运行&lt;code&gt;eval $(magnum cluster-config &amp;lt;cluster-name&amp;gt;)&lt;/code&gt; 获取证书，然后就可以正常运行 kubectl 的各种命令了。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;确认 k8s API 功能正常
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;kubectl get pods --all-namespaces&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;确认个 minion/worker 节点状态正常
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;fedora@k8scluster-llrejue2ujzv-master-0 ~&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;$ kubectl get nodes&lt;br /&gt;NAME                               STATUS     ROLES     AGE       VERSION&lt;br /&gt;k8scluster-llrejue2ujzv-minion-0   Ready      &amp;lt;none&amp;gt;    3d        v1.9.3&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建一个pod, 确认能够可以正常分配 IP，可以用下面的 yaml 文件创建一个 pod，
把下面的内容保存为文件 busybox.yaml
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;apiVersion: v1&lt;br /&gt;kind: Pod&lt;br /&gt;metadata:&lt;br /&gt;  name: busybox&lt;br /&gt;  namespace: default&lt;br /&gt;spec:&lt;br /&gt;  containers:&lt;br /&gt;  - image: busybox&lt;br /&gt;    command:&lt;br /&gt;      - sleep&lt;br /&gt;      - &lt;span class=&#34;s2&#34;&gt;&amp;quot;3600&amp;quot;&lt;/span&gt;&lt;br /&gt;    imagePullPolicy: IfNotPresent&lt;br /&gt;    name: busybox&lt;br /&gt;restartPolicy: Always&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br /&gt;
接下来运行 kubectl create -f busybox.yaml
即可成功创建一个 pod&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过 Magnum 按照的k8s 默认会安装 CoreDNS, 运行下面的代码可以验证 DNS 是否正常
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;kubectl &lt;span class=&#34;nb&#34;&gt;exec&lt;/span&gt; busybox nslookup kubernetes&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br /&gt;
未完待续...&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;      
          </content>
  </entry>
  

</feed>